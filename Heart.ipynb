{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Khởi tạo Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "findspark.find()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Heart Disease\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Đọc và load tập dữ liệu Boston housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---------+-------------+----------+------+---------------+------------+--------+-------+-----+-----+-----+---------+-------+----------+\n",
      "|male|age|education|currentSmoker|cigsPerDay|BPMeds|prevalentStroke|prevalentHyp|diabetes|totChol|sysBP|diaBP|  BMI|heartRate|glucose|TenYearCHD|\n",
      "+----+---+---------+-------------+----------+------+---------------+------------+--------+-------+-----+-----+-----+---------+-------+----------+\n",
      "|   1| 39|        4|            0|         0|     0|              0|           0|       0|    195|106.0| 70.0|26.97|       80|     77|         0|\n",
      "|   0| 46|        2|            0|         0|     0|              0|           0|       0|    250|121.0| 81.0|28.73|       95|     76|         0|\n",
      "|   1| 48|        1|            1|        20|     0|              0|           0|       0|    245|127.5| 80.0|25.34|       75|     70|         0|\n",
      "|   0| 61|        3|            1|        30|     0|              0|           1|       0|    225|150.0| 95.0|28.58|       65|    103|         1|\n",
      "|   0| 46|        3|            1|        23|     0|              0|           0|       0|    285|130.0| 84.0| 23.1|       85|     85|         0|\n",
      "|   0| 43|        2|            0|         0|     0|              0|           1|       0|    228|180.0|110.0| 30.3|       77|     99|         0|\n",
      "|   0| 63|        1|            0|         0|     0|              0|           0|       0|    205|138.0| 71.0|33.11|       60|     85|         1|\n",
      "|   0| 45|        2|            1|        20|     0|              0|           0|       0|    313|100.0| 71.0|21.68|       79|     78|         0|\n",
      "|   1| 52|        1|            0|         0|     0|              0|           1|       0|    260|141.5| 89.0|26.36|       76|     79|         0|\n",
      "|   1| 43|        1|            1|        30|     0|              0|           1|       0|    225|162.0|107.0|23.61|       93|     88|         0|\n",
      "|   0| 50|        1|            0|         0|     0|              0|           0|       0|    254|133.0| 76.0|22.91|       75|     76|         0|\n",
      "|   0| 43|        2|            0|         0|     0|              0|           0|       0|    247|131.0| 88.0|27.64|       72|     61|         0|\n",
      "|   1| 46|        1|            1|        15|     0|              0|           1|       0|    294|142.0| 94.0|26.31|       98|     64|         0|\n",
      "|   0| 41|        3|            0|         0|     1|              0|           1|       0|    332|124.0| 88.0|31.31|       65|     84|         0|\n",
      "|   0| 39|        2|            1|         9|     0|              0|           0|       0|    226|114.0| 64.0|22.35|       85|     NA|         0|\n",
      "|   0| 38|        2|            1|        20|     0|              0|           1|       0|    221|140.0| 90.0|21.35|       95|     70|         1|\n",
      "|   1| 48|        3|            1|        10|     0|              0|           1|       0|    232|138.0| 90.0|22.37|       64|     72|         0|\n",
      "|   0| 46|        2|            1|        20|     0|              0|           0|       0|    291|112.0| 78.0|23.38|       80|     89|         1|\n",
      "|   0| 38|        2|            1|         5|     0|              0|           0|       0|    195|122.0| 84.5|23.24|       75|     78|         0|\n",
      "|   1| 41|        2|            0|         0|     0|              0|           0|       0|    195|139.0| 88.0|26.88|       85|     65|         0|\n",
      "+----+---+---------+-------------+----------+------+---------------+------------+--------+-------+-----+-----+-----+---------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "heartDF = (spark.read\n",
    "            .option(\"HEADER\", True)\n",
    "            .option(\"inferSchema\", True)\n",
    "            .csv(\"C:/Users/Phung Huy Hoang/Downloads/framingham.csv\")\n",
    "           )\n",
    "\n",
    "heartDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tập dữ liệu Framingham Heart Disease\n",
    "`male`: 1 là `nam`, 0 là `nữ`\n",
    "\n",
    "`age`: Độ tuổi\n",
    "\n",
    "`education`: Trình độ giáo dục\n",
    "\n",
    "`currentSmoker`: Có đang hút thuốc hay không\n",
    "\n",
    "`cigsPerDay`: Số điếu thuốc mỗi ngày\n",
    "\n",
    "`BPMeds`: Có đang dùng thuốc huyết áp hay không\n",
    "\n",
    "`prevalentStroke`: Trước đó có bị đột quỵ hay không\n",
    "\n",
    "`prevalentHyp`: Có bị tăng huyết áp hay không\n",
    "\n",
    "`diabetes`: Có bị tiểu đường hay không\n",
    "\n",
    "`totChol`: Tổng mức cholesterol\n",
    "\n",
    "`sysBP`: Huyết áp tâm thu\n",
    "\n",
    "`diaBP`: Huyết áp tâm trương\n",
    "\n",
    "`BMI`: Chỉ số khối cơ thể\n",
    "\n",
    "`heartRate`: Nhịp tim\n",
    "\n",
    "`glucose`: Lượng đường trong máu\n",
    "\n",
    "Biến dự báo:\n",
    "\n",
    "`TenYearCHD`: Nguy cơ mắc bệnh tim mạch vành trong 10 năm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xem các loại biến trong tập dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('male', 'int'),\n",
       " ('age', 'int'),\n",
       " ('education', 'string'),\n",
       " ('currentSmoker', 'int'),\n",
       " ('cigsPerDay', 'string'),\n",
       " ('BPMeds', 'string'),\n",
       " ('prevalentStroke', 'int'),\n",
       " ('prevalentHyp', 'int'),\n",
       " ('diabetes', 'int'),\n",
       " ('totChol', 'string'),\n",
       " ('sysBP', 'double'),\n",
       " ('diaBP', 'double'),\n",
       " ('BMI', 'string'),\n",
       " ('heartRate', 'string'),\n",
       " ('glucose', 'string'),\n",
       " ('TenYearCHD', 'int')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heartDF.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các biến được lưu trong file CSV dưới dạng số. Tuy nhiên khi đọc vào Spark, một số biến lại trở thành dạng `string` bởi vì trong biến có chứa giá trị `'NA'`.\n",
    "\n",
    "Những biến nào chứa giá trị `'NA'` sẽ thay bằng giá trị ... (chưa nghĩ ra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('male', 'int'),\n",
       " ('age', 'int'),\n",
       " ('education', 'int'),\n",
       " ('currentSmoker', 'int'),\n",
       " ('cigsPerDay', 'int'),\n",
       " ('BPMeds', 'int'),\n",
       " ('prevalentStroke', 'int'),\n",
       " ('prevalentHyp', 'int'),\n",
       " ('diabetes', 'int'),\n",
       " ('totChol', 'int'),\n",
       " ('sysBP', 'double'),\n",
       " ('diaBP', 'double'),\n",
       " ('BMI', 'double'),\n",
       " ('heartRate', 'int'),\n",
       " ('glucose', 'int'),\n",
       " ('TenYearCHD', 'int')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "heartDF = heartDF.withColumn('education', regexp_replace('education', 'NA', '-9999')).\\\n",
    "    withColumn('education', col(\"education\").cast('int')).\\\n",
    "    withColumn('cigsPerDay', regexp_replace('cigsPerDay', 'NA', '-9999')).\\\n",
    "    withColumn('cigsPerDay', col(\"cigsPerDay\").cast('int')).\\\n",
    "    withColumn('BPMeds', regexp_replace('BPMeds', 'NA', '-9999')).\\\n",
    "    withColumn('BPMeds', col(\"BPMeds\").cast('int')).\\\n",
    "    withColumn('totChol', regexp_replace('totChol', 'NA', '-9999')).\\\n",
    "    withColumn('totChol', col(\"totChol\").cast('int')).\\\n",
    "    withColumn('BMI', regexp_replace('BMI', 'NA', '-9999')).\\\n",
    "    withColumn('BMI', col(\"BMI\").cast('double')).\\\n",
    "    withColumn('heartRate', regexp_replace('heartRate', 'NA', '-9999')).\\\n",
    "    withColumn('heartRate', col(\"heartRate\").cast('int')).\\\n",
    "    withColumn('glucose', regexp_replace('glucose', 'NA', '-9999')).\\\n",
    "    withColumn('glucose', col(\"glucose\").cast('int'))\n",
    "\n",
    "heartDF.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chia dữ liệu thành train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---------+-------------+----------+------+---------------+------------+--------+-------+-----+-----+-----+---------+-------+----------+\n",
      "|male|age|education|currentSmoker|cigsPerDay|BPMeds|prevalentStroke|prevalentHyp|diabetes|totChol|sysBP|diaBP|  BMI|heartRate|glucose|TenYearCHD|\n",
      "+----+---+---------+-------------+----------+------+---------------+------------+--------+-------+-----+-----+-----+---------+-------+----------+\n",
      "|   1| 39|        4|            0|         0|     0|              0|           0|       0|    195|106.0| 70.0|26.97|       80|     77|         0|\n",
      "|   0| 46|        2|            0|         0|     0|              0|           0|       0|    250|121.0| 81.0|28.73|       95|     76|         0|\n",
      "|   1| 48|        1|            1|        20|     0|              0|           0|       0|    245|127.5| 80.0|25.34|       75|     70|         0|\n",
      "|   0| 61|        3|            1|        30|     0|              0|           1|       0|    225|150.0| 95.0|28.58|       65|    103|         1|\n",
      "|   0| 46|        3|            1|        23|     0|              0|           0|       0|    285|130.0| 84.0| 23.1|       85|     85|         0|\n",
      "|   0| 43|        2|            0|         0|     0|              0|           1|       0|    228|180.0|110.0| 30.3|       77|     99|         0|\n",
      "|   0| 63|        1|            0|         0|     0|              0|           0|       0|    205|138.0| 71.0|33.11|       60|     85|         1|\n",
      "|   0| 45|        2|            1|        20|     0|              0|           0|       0|    313|100.0| 71.0|21.68|       79|     78|         0|\n",
      "|   1| 52|        1|            0|         0|     0|              0|           1|       0|    260|141.5| 89.0|26.36|       76|     79|         0|\n",
      "|   1| 43|        1|            1|        30|     0|              0|           1|       0|    225|162.0|107.0|23.61|       93|     88|         0|\n",
      "|   0| 50|        1|            0|         0|     0|              0|           0|       0|    254|133.0| 76.0|22.91|       75|     76|         0|\n",
      "|   0| 43|        2|            0|         0|     0|              0|           0|       0|    247|131.0| 88.0|27.64|       72|     61|         0|\n",
      "|   1| 46|        1|            1|        15|     0|              0|           1|       0|    294|142.0| 94.0|26.31|       98|     64|         0|\n",
      "|   0| 41|        3|            0|         0|     1|              0|           1|       0|    332|124.0| 88.0|31.31|       65|     84|         0|\n",
      "|   0| 39|        2|            1|         9|     0|              0|           0|       0|    226|114.0| 64.0|22.35|       85|  -9999|         0|\n",
      "|   0| 38|        2|            1|        20|     0|              0|           1|       0|    221|140.0| 90.0|21.35|       95|     70|         1|\n",
      "|   1| 48|        3|            1|        10|     0|              0|           1|       0|    232|138.0| 90.0|22.37|       64|     72|         0|\n",
      "|   0| 46|        2|            1|        20|     0|              0|           0|       0|    291|112.0| 78.0|23.38|       80|     89|         1|\n",
      "|   0| 38|        2|            1|         5|     0|              0|           0|       0|    195|122.0| 84.5|23.24|       75|     78|         0|\n",
      "|   1| 41|        2|            0|         0|     0|              0|           0|       0|    195|139.0| 88.0|26.88|       85|     65|         0|\n",
      "+----+---+---------+-------------+----------+------+---------------+------------+--------+-------+-----+-----+-----+---------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(trainDF, testDF) = heartDF.randomSplit([.8, .2], seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biến đổi train data theo định dạng của Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            features|TenYearCHD|\n",
      "+--------------------+----------+\n",
      "|[0.0,32.0,2.0,1.0...|         0|\n",
      "|(15,[1,2,9,10,11,...|         0|\n",
      "|[0.0,33.0,2.0,1.0...|         0|\n",
      "+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['male','age','education','currentSmoker',\\\n",
    "                                       'cigsPerDay','BPMeds','prevalentStroke',\\\n",
    "                                       'prevalentHyp','diabetes','totChol','sysBP',\\\n",
    "                                       'diaBP','BMI','heartRate', 'glucose'],\n",
    "                            outputCol='features') \n",
    "assembler_train = assembler.transform(trainDF)\n",
    "final_train = assembler_train.select('features','TenYearCHD')\n",
    "final_train.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Decision Tree\n",
    "\n",
    "## Huấn luyện mô hình Decision Tree Regressor trên train data\n",
    "\n",
    "**1.1** Huấn luyện mô hình trên `final_train` với `DecisionTreeRegressor` với `labelCol` là `'TenYearCHD'` và `featuresCol` là `'features'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o738.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 27.0 failed 1 times, most recent failure: Lost task 0.0 in stage 27.0 (TID 27, 192.168.70.1, executor driver): org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$2458/1145457780: (struct<male_double_VectorAssembler_a0a845897161:double,age_double_VectorAssembler_a0a845897161:double,education_double_VectorAssembler_a0a845897161:double,currentSmoker_double_VectorAssembler_a0a845897161:double,cigsPerDay_double_VectorAssembler_a0a845897161:double,BPMeds_double_VectorAssembler_a0a845897161:double,prevalentStroke_double_VectorAssembler_a0a845897161:double,prevalentHyp_double_VectorAssembler_a0a845897161:double,diabetes_double_VectorAssembler_a0a845897161:double,totChol_double_VectorAssembler_a0a845897161:double,sysBP:double,diaBP:double,BMI:double,heartRate_double_VectorAssembler_a0a845897161:double,glucose_double_VectorAssembler_a0a845897161:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\r\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\r\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\r\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1181)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\r\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\r\n\t... 25 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2194)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1183)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1176)\r\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\r\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor.$anonfun$train$1(DecisionTreeRegressor.scala:126)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:114)\r\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:44)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:150)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$2458/1145457780: (struct<male_double_VectorAssembler_a0a845897161:double,age_double_VectorAssembler_a0a845897161:double,education_double_VectorAssembler_a0a845897161:double,currentSmoker_double_VectorAssembler_a0a845897161:double,cigsPerDay_double_VectorAssembler_a0a845897161:double,BPMeds_double_VectorAssembler_a0a845897161:double,prevalentStroke_double_VectorAssembler_a0a845897161:double,prevalentHyp_double_VectorAssembler_a0a845897161:double,diabetes_double_VectorAssembler_a0a845897161:double,totChol_double_VectorAssembler_a0a845897161:double,sysBP:double,diaBP:double,BMI:double,heartRate_double_VectorAssembler_a0a845897161:double,glucose_double_VectorAssembler_a0a845897161:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\r\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\r\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\r\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1181)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\r\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-504e0fb82354>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabelCol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'TenYearCHD'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeaturesCol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'features'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdtModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    127\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    316\u001b[0m         \"\"\"\n\u001b[0;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o738.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 27.0 failed 1 times, most recent failure: Lost task 0.0 in stage 27.0 (TID 27, 192.168.70.1, executor driver): org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$2458/1145457780: (struct<male_double_VectorAssembler_a0a845897161:double,age_double_VectorAssembler_a0a845897161:double,education_double_VectorAssembler_a0a845897161:double,currentSmoker_double_VectorAssembler_a0a845897161:double,cigsPerDay_double_VectorAssembler_a0a845897161:double,BPMeds_double_VectorAssembler_a0a845897161:double,prevalentStroke_double_VectorAssembler_a0a845897161:double,prevalentHyp_double_VectorAssembler_a0a845897161:double,diabetes_double_VectorAssembler_a0a845897161:double,totChol_double_VectorAssembler_a0a845897161:double,sysBP:double,diaBP:double,BMI:double,heartRate_double_VectorAssembler_a0a845897161:double,glucose_double_VectorAssembler_a0a845897161:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\r\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\r\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\r\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1181)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\r\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\r\n\t... 25 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2194)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1183)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1176)\r\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\r\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor.$anonfun$train$1(DecisionTreeRegressor.scala:126)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:114)\r\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:44)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:150)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$2458/1145457780: (struct<male_double_VectorAssembler_a0a845897161:double,age_double_VectorAssembler_a0a845897161:double,education_double_VectorAssembler_a0a845897161:double,currentSmoker_double_VectorAssembler_a0a845897161:double,cigsPerDay_double_VectorAssembler_a0a845897161:double,BPMeds_double_VectorAssembler_a0a845897161:double,prevalentStroke_double_VectorAssembler_a0a845897161:double,prevalentHyp_double_VectorAssembler_a0a845897161:double,diabetes_double_VectorAssembler_a0a845897161:double,totChol_double_VectorAssembler_a0a845897161:double,sysBP:double,diaBP:double,BMI:double,heartRate_double_VectorAssembler_a0a845897161:double,glucose_double_VectorAssembler_a0a845897161:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\r\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\r\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\r\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1181)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2193)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\r\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor(labelCol = 'TenYearCHD', featuresCol = 'features')\n",
    "dtModel = dt.fit(final_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
